import { ObservableValue, DefaultObservableValue } from "@opendaw/lib-std"
import { LLMProvider, Message, ProviderConfig, LLMTool } from "./LLMProvider"

// [GEMINI 3 SPEC] Protocol Buffers & Types
type GeminiPart =
    | { text: string; thought?: boolean }
    | { functionCall: { name: string; args: Record<string, unknown> }; thoughtSignature?: string }
    | { functionResponse: { name: string; response: Record<string, unknown> } }
    | { inlineData: { mimeType: string; data: string } }

type GeminiContent = { role: "user" | "model"; parts: GeminiPart[] }
type GeminiTool = { functionDeclarations: unknown[] }
type GeminiRequest = {
    contents: GeminiContent[];
    system_instruction?: { parts: GeminiPart[] };
    tools?: GeminiTool[];
    generationConfig?: {
        responseModalities?: string[];
        thinkingConfig?: {
            thinkingLevel: "minimal" | "low" | "medium" | "high";
        };
        mediaResolution?: "MEDIA_RESOLUTION_LOW" | "MEDIA_RESOLUTION_MEDIUM" | "MEDIA_RESOLUTION_HIGH" | "MEDIA_RESOLUTION_ULTRA_HIGH";
    };
}

export type KeyStatus = 'ready' | 'exhausted' | 'invalid' | 'unknown'

export interface KeyInfo {
    key: string
    status: KeyStatus
    isActive: boolean
}

/**
 * [GEMINI 3 CORTEX]
 * The "System 2" Brain.
 * 
 * Major Upgrades:
 * 1. Echo Protocol: Captures and injects `thoughtSignature` to maintain reasoning state.
 * 2. Thinking Levels: Explicit control over "Cognitive Depth".
 * 3. State Machine Parser: Handles interleaved Thoughts/Tools and Binary Blobs.
 * 4. Nano Banana: Ultra-High resolution visual routing.
 */
export class Gemini3Provider implements LLMProvider {
    readonly id = "gemini-3" // Distinct ID for safety
    readonly manifest = {
        name: "Gemini 3 (Cognitive)",
        description: "Review Mode Only. Features 'Thinking' & 'Echo Protocol'.",
        icon: "üß†",
        getKeyUrl: "https://aistudio.google.com/app/apikey",
        docsUrl: "https://ai.google.dev/gemini-api/docs/thinking"
    }
    readonly requiresKey = true
    readonly requiresUrl = false

    private keyRing: string[] = []
    private keyStatus: KeyStatus[] = []
    private activeKeyIndex: number = 0
    private config: ProviderConfig | undefined

    // [STATE] The Cognitive Cookie
    // We must persist this between turns for the SAME session.
    private lastThoughtSignature: string | null = null

    // Models
    private static readonly REASONING_MODEL = "gemini-3-flash-preview"
    private static readonly VISION_MODEL = "gemini-3-pro-image-preview"

    // [A2UI PERSISTENCE] Storage key for cross-refresh signature persistence
    private static readonly SIG_STORAGE_KEY = "odie_gemini3_thought_sig"

    configure(config: ProviderConfig) {
        this.config = config
        this.keyRing = config.keyLibrary || []
        this.keyStatus = this.keyRing.map(() => 'unknown' as KeyStatus)
        if (config.apiKey && !this.keyRing.includes(config.apiKey)) {
            this.keyRing.unshift(config.apiKey)
            this.keyStatus.unshift('unknown')
        }
        this.activeKeyIndex = 0

        // [A2UI FIX] Restore thought signature from localStorage if we don't have one
        if (!this.lastThoughtSignature) {
            try {
                const stored = localStorage.getItem(Gemini3Provider.SIG_STORAGE_KEY)
                if (stored) {
                    this.lastThoughtSignature = stored
                    console.log("üß† [Echo Protocol] Restored signature from storage.")
                }
            } catch (e) {
                // Ignore storage errors (e.g., private browsing)
            }
        }
    }

    getKeyStatuses(): KeyInfo[] {
        return this.keyRing.map((key, idx) => ({
            key: '‚Ä¢‚Ä¢‚Ä¢' + key.slice(-4),
            status: this.keyStatus[idx] || 'unknown',
            isActive: idx === this.activeKeyIndex
        }))
    }

    private get currentKey(): string {
        return this.keyRing[this.activeKeyIndex] || ""
    }

    private markCurrentKey(status: KeyStatus) {
        this.keyStatus[this.activeKeyIndex] = status
    }

    private rotateKey(): boolean {
        if (this.keyRing.length <= 1) return false
        this.markCurrentKey('exhausted')
        let attempts = 0
        while (attempts < this.keyRing.length) {
            this.activeKeyIndex = (this.activeKeyIndex + 1) % this.keyRing.length
            const nextStatus = this.keyStatus[this.activeKeyIndex]
            if (nextStatus !== 'exhausted' && nextStatus !== 'invalid') return true
            attempts++
        }
        return false
    }

    // [ROUTER] Detecting Visual Intent
    private isImageRequest(messages: Message[]): boolean {
        const lastUserMsg = [...messages].reverse().find(m => m.role === "user")
        if (!lastUserMsg?.content) return false
        const text = lastUserMsg.content.toLowerCase()
        const imagePatterns = [
            /\b(visualize|draw|render|sketch|schematic|diagram|blueprint|infographic|chart|graph)\b/i,
            /\b(show me|generate image|create image|make an image)\b/i,
            /\b(create a|generate a)\b.*\b(visual|image|picture|photo|infographic|diagram)\b/i
        ]
        return imagePatterns.some(pattern => pattern.test(text))
    }

    // [ANTIGRAVITY] Validate API Connection (matching GeminiProvider behavior)
    async validate(): Promise<{ ok: boolean, message: string, status?: 'valid' | 'exhausted' | 'invalid' }> {
        if (this.keyRing.length === 0) return { ok: false, message: "No API Keys provided", status: 'invalid' }

        try {
            // Validate the CURRENT key using the Reasoning Model
            const url = `https://generativelanguage.googleapis.com/v1beta/models/${Gemini3Provider.REASONING_MODEL}?key=${this.currentKey}`
            const response = await fetch(url)

            if (response.ok) {
                return { ok: true, message: `‚úì Valid (Library: ${this.keyRing.length} Key${this.keyRing.length > 1 ? 's' : ''})`, status: 'valid' }
            }

            // Parse error for smart classification
            const err = await response.json()
            const rawMsg = err.error?.message || response.statusText

            // Quota Exhausted
            if (response.status === 429 || rawMsg.includes('Quota') || rawMsg.includes('RESOURCE_EXHAUSTED')) {
                return { ok: false, message: "‚è≥ Daily quota exhausted. Resets at midnight PT.", status: 'exhausted' }
            }

            // Invalid API Key
            if (rawMsg.includes('API key not valid') || rawMsg.includes('API_KEY_INVALID')) {
                return { ok: false, message: "üîë Invalid API key. Please check and try again.", status: 'invalid' }
            }

            return { ok: false, message: rawMsg, status: 'invalid' }
        } catch (e) {
            return { ok: false, message: (e instanceof Error) ? e.message : String(e), status: 'invalid' }
        }
    }

    streamChat(
        messages: Message[],
        context?: unknown,
        tools?: LLMTool[],
        onFinal?: (msg: Message) => void,
        onStatusChange?: (status: string, model?: string) => void
    ): ObservableValue<string> {
        const responseText = new DefaultObservableValue<string>("")

        if (this.keyRing.length === 0) {
            responseText.setValue("‚ö†Ô∏è Configuration Error: Please add keys to your Gemini Key Library.")
            return responseText
        }

        const run = async () => {
            // [GEMINI 3] Retry Logic (Simplified for V2)
            try {
                if (onStatusChange) onStatusChange("Thinking...", "Gemini ‚Ä¢ 3 System 2")
                await this.executeRequest(messages, context, tools, responseText, onFinal, onStatusChange)
            } catch (e: any) {
                const err = e.message || String(e)
                if (err.includes("429") || err.includes("Quota")) {
                    if (this.rotateKey()) {
                        await run() // Retry with new key
                        return
                    }
                }
                responseText.setValue(`üß† **Cognitive Error**\n\n${err}`)
            }
        }

        this.runStream(run)
        return responseText
    }

    private async executeRequest(
        messages: Message[],
        context: unknown,
        tools: LLMTool[] | undefined,
        responseText: DefaultObservableValue<string>,
        onFinal?: (msg: Message) => void,
        onStatusChange?: (status: string, model?: string) => void
    ) {
        if (onStatusChange) onStatusChange("Thinking...", "Gemini 3 Cognitive")

        // [1] CONVERT MESSAGES & INJECT SIGNATURE
        const contents = messages
            .filter(m => m.role !== 'system')
            .map(m => {
                const parts: GeminiPart[] = []

                // [ECHO REHYDRATION]
                let sig = (m.customData?.thoughtSignature || m.customData?.thought_signature) as string | undefined

                // [FIX] Use live state if history lacks it (for current turn continuity)
                if (!sig && m.role === 'model' && this.lastThoughtSignature) {
                    sig = this.lastThoughtSignature
                }

                if (m.role === 'function') {
                    // Tool response turn
                    try {
                        const response = typeof m.content === 'string' ? JSON.parse(m.content) : m.content
                        parts.push({
                            functionResponse: {
                                name: m.name || "unknown",
                                response: response || {}
                            }
                        })
                    } catch (e) {
                        parts.push({ text: m.content }) // Fallback
                    }
                } else {
                    if (m.content) parts.push({ text: m.content })
                }

                if (m.tool_calls) {
                    m.tool_calls.forEach((tc, classIdx) => {
                        const part: GeminiPart = {
                            functionCall: { name: tc.name, args: tc.arguments }
                        }
                        // [ECHO PROTOCOL] Only the FIRST function call in a block should contain the signature
                        if (sig && classIdx === 0) {
                            (part as any).thoughtSignature = sig
                        }
                        parts.push(part)
                    })
                }

                // [FIX] Gemini API uses 'user' for both user messages and function responses
                const role = m.role === 'model' || (m.role as any) === 'assistant' ? 'model' : 'user'
                return { role, parts }
            }) as GeminiContent[]

        // [2] ROUTING & CONFIG
        const wantsImage = this.isImageRequest(messages)
        const activeModel = wantsImage ? Gemini3Provider.VISION_MODEL : Gemini3Provider.REASONING_MODEL

        // [GEMINI 3] RESTORED: VISUAL DICTIONARY (Domain Knowledge)
        const VISUAL_DICTIONARY: Record<string, string> = {
            "distortion": "VISUAL_DEFINITION: Waveform Clipping. Square tops. TEXT_LABEL: 'Distortion'.",
            "saturation": "VISUAL_DEFINITION: Soft clipping. Rounded square tops. Warm colors. TEXT_LABEL: 'Saturation'.",
            "sawtooth": "VISUAL_DEFINITION: Sharp, jagged triangular wave. Linear rise, instantaneous drop. TEXT_LABEL: 'Sawtooth'.",
            "sine": "VISUAL_DEFINITION: Smooth, perfect curve. No sharp angles. TEXT_LABEL: 'Sine'.",
            "square": "VISUAL_DEFINITION: 90-degree angles. Flat top, flat bottom. Digital look. TEXT_LABEL: 'Square'.",
            "compression": "VISUAL_DEFINITION: Dynamic range reduction. The peaks are squashed down. TEXT_LABEL: 'Compression'.",
            "eq": "VISUAL_DEFINITION: Frequency Spectrum Graph. 20Hz to 20kHz. Bell curves. TEXT_LABEL: 'EQ'.",
            "reverb": "VISUAL_DEFINITION: Dense reflections fading over time. Diffusion. Cloud-like. TEXT_LABEL: 'Reverb'.",
            "delay": "VISUAL_DEFINITION: Distinct, repeating echoes fading out. Rhythmic. TEXT_LABEL: 'Delay'.",
            "phaser": "VISUAL_DEFINITION: Sweeping peaks and notches (Comb Filter) moving over time. TEXT_LABEL: 'Phaser'.",
            "lfo": "VISUAL_DEFINITION: Low Frequency Oscillator. Slow moving wave modulating a parameter. TEXT_LABEL: 'LFO'.",
            "low pass": "VISUAL_DEFINITION: Filter Curve. High frequencies rolled off. Downward slope. TEXT_LABEL: 'Low Pass'.",
            "high pass": "VISUAL_DEFINITION: Filter Curve. Low frequencies rolled off. Upward slope. TEXT_LABEL: 'High Pass'."
        }

        let conceptVisuals = ""
        // Scan the *User's Request* for dictionary keys
        const lastUserMsgForScan = contents.slice().reverse().find(m => m.role === 'user')
        let lastUserText = ""
        if (lastUserMsgForScan && lastUserMsgForScan.parts[0] && 'text' in lastUserMsgForScan.parts[0]) {
            lastUserText = (lastUserMsgForScan.parts[0] as any).text.toLowerCase()
        }

        if (lastUserText) {
            const foundConcepts: string[] = []
            Object.keys(VISUAL_DICTIONARY).forEach(key => {
                if (lastUserText.includes(key)) foundConcepts.push(VISUAL_DICTIONARY[key])
            })
            if (foundConcepts.length > 0) conceptVisuals = "\nVISUAL DICTIONARY INJECTIONS:\n" + foundConcepts.join("\n")
        }

        let geminiTools: GeminiTool[] | undefined
        if (tools && !wantsImage) {
            geminiTools = [{
                functionDeclarations: tools.map(t => ({
                    name: t.name,
                    description: t.description,
                    parameters: t.parameters
                }))
            }]
        }

        // [3] STRICT SYSTEM INSTRUCTION (Anti-Hallucination)
        // [GEMINI 3] FIX: Inject the CORE IDENTITY from the messages array
        const systemMsg = messages.find(m => m.role === "system")
        const baseIdentity = systemMsg ? systemMsg.content : ""

        const systemPromptText = context ? JSON.stringify(context) : ""
        const toolProtocol = `
[TOOL PROTOCOL]
1. üéõÔ∏è APP CONTROL: You have access to a suite of DAW tools. USE THEM immediately when requested:
   - 'add_track', 'add_effect', 'add_instrument'
   - 'set_device_param', 'transport_control'
   - 'save_project', 'import_audio'
   Do not describe the action‚ÄîEXECUTE IT.

2. üé® UI GENERATION: For visual components, you MUST use the 'render_widget' tool.
   - Schema: { component: "smart_knob" | "comparison_table" ..., data: { ... } }

3. ‚õî RESTRICTIONS: DO NOT invent tools outside of the provided function declarations.`

        let systemInstruction = {
            parts: [{ text: baseIdentity + "\n\n" + systemPromptText + toolProtocol }]
        }

        // [GEMINI 3] RESTORED: MULTI-STAGE REASONING PIPELINE (Visual Director)
        let finalContents = contents

        if (wantsImage) {
            // STEP 1: REASONING (Gemini 3 Flash)
            if (onStatusChange) onStatusChange("Reasoning...", "Gemini ‚Ä¢ 3 Flash")

            const DIRECTOR_PROMPT = `
            [ROLE: EXPERT AUDIO VISUALIZATION DIRECTOR]
            You are an expert in Audio Engineering, Music Theory, and Digital Signal Processing.
            The user wants to LEARN. Do not generate "abstract art". Generate "Educational Schematic" and "Diagrams".
            We need clear signal flows, vector-style layouts, and clean visual explanations.

            [PROJECT CONTEXT]
            ${systemPromptText.substring(0, 1000)}...

            [USER REQUEST]
            ${lastUserText}

            [VISUAL DICTIONARY]
            ${conceptVisuals}

            [TASK]
            1. Analyze the technical concept (e.g., signal flow, frequency spectrum, sidechaining).
            2. Design a VISUAL DIAGRAM or SCHEMATIC that explains it.
            3. Output ONLY the detailed prompt for the layout/renderer.
            4. KEYWORDS TO USE: "Schematic", "Diagram", "Infographic", "Cross-section", "Signal Flow", "Vector Style", "Whiteboard", "Technical Illustration".
            5. NEGATIVE PROMPTS (Avoid): "Photorealistic", "Abstract", "Artistic", "Vague", "Generic", "3D Render", "Glossy".
            6. Do not output JSON. Just the visual description.
            `

            console.log("üß† [Gemini 3 Visual Reasoning] Reasoning...")
            const reasonUrl = `https://generativelanguage.googleapis.com/v1beta/models/${Gemini3Provider.REASONING_MODEL}:generateContent?key=${this.currentKey}`

            const reasonRes = await fetch(reasonUrl, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({
                    contents: [{ role: "user", parts: [{ text: DIRECTOR_PROMPT }] }]
                })
            })

            let visualBlueprint = lastUserText // Fallback
            if (reasonRes.ok) {
                const reasonData = await reasonRes.json()
                const blueprint = reasonData.candidates?.[0]?.content?.parts?.[0]?.text
                if (blueprint) {
                    visualBlueprint = blueprint + " Style: Technical Schematic, Clean Lines, Bauhaus, Educational."
                    console.log("üß† [Gemini 3 Visual Reasoning] Blueprint Created:", visualBlueprint.substring(0, 50) + "...")
                }
            } else {
                console.error("üß† [Gemini 3 Visual Reasoning] Failed to reason. Fallback to raw request.")
            }

            // STEP 2: RENDERING (Nano Banana Pro)
            if (onStatusChange) onStatusChange("Painting...", "Nano Banana Pro")

            const VISUAL_STYLE_GUIDE = `
            [SYSTEM_INSTRUCTION: VISUAL_RENDERER_PROCESS]
            MODE: NATIVE_IMAGE_GENERATION_V5 (Nano Banana)
            STATUS: ACTIVE
            
            CMD: GENERATE_PIXEL_DATA
            OUTPUT: IMAGE/PNG (Native Inline Data)
            
            Visual Standards:
            - **Style: Technical, Schematic, Educational, Clean, High Contrast.**
            - **Palette:** Deep Void (#0f172a) used for background only. Vibrant Signal Colors (Neon Blue, Hot Pink, Electric Green).
            - **Typography:** STRICT, Clean, Spelled Correctly.

            CRITICAL PROTOCOL:
            1. Fulfill the VISUAL BLUEPRINT exactly.
            2. DO NOT output JSON, XML, or "dalle" tool calls.
            3. DO NOT output "thought" blocks or reasoning text.
            4. You are DIRECTLY connected to the pixel engine.
            `

            // Override System Instruction for the Vision Model
            systemInstruction = { parts: [{ text: VISUAL_STYLE_GUIDE }] }

            // Rebuild contents to be SINGLE SHOT using the BLUEPRINT
            finalContents = [{
                role: "user",
                parts: [{
                    text: `[MANDATORY: GENERATE IMAGE]\nVISUAL BLUEPRINT:\n"${visualBlueprint}"\n\nGUIDANCE: detailed, 4k resolution, technical diagram.`
                }]
            }] as any
        }

        // [3] EXECUTE STREAM
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${activeModel}:streamGenerateContent?key=${this.currentKey}`

        console.log(`üß† [Gemini 3] Requesting ${activeModel}...`)

        const requestPayload: GeminiRequest = {
            contents: finalContents,
            system_instruction: systemInstruction,
            tools: geminiTools,
            generationConfig: wantsImage ? undefined : {
                thinkingConfig: {
                    thinkingLevel: this.config?.thinkingLevel || "low"
                }
                // [GEMINI 3] FIX: Removed 'mediaResolution' (API 400 Invalid Argument)
            }
        }

        console.log("üîç [Gemini 3 Debug] Full Request Payload:", JSON.stringify(requestPayload, null, 2))

        // [PROMPT TRACER] Verification that the "Real" system is engaged
        if (systemInstruction && systemInstruction.parts) {
            const sysText = systemInstruction.parts[0].text
            console.groupCollapsed("üß† [Gemini 3 Prompt Tracer]")
            console.log("üìù Base Identity:", sysText.slice(0, 50) + "...")
            console.log("üïµÔ∏è Context Scanner:", conceptVisuals ? "ACTIVE (Injected)" : "INACTIVE (No keywords)")
            console.log("üé® Visual Reasoning:", wantsImage ? "ACTIVE (Reasoning)" : "STANDBY")
            console.log("üõ†Ô∏è Tool Protocol:", sysText.includes("[TOOL PROTOCOL]") ? "VERIFIED" : "MISSING")
            console.log("üì¶ Full System Prompt:", sysText)
            console.groupEnd()
        }

        const response = await fetch(url, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify(requestPayload)
        })

        if (!response.ok) throw new Error(`API ${response.status}: ${await response.text()}`)
        if (!response.body) throw new Error("No Body")

        // [4] JSONL STREAMING PARSER (A2UI Compliant)
        // The API returns newline-delimited JSON objects wrapped in an array.
        // We buffer incoming data and split by valid JSON objects.
        const reader = response.body.getReader()
        const decoder = new TextDecoder("utf-8")
        let buffer = ""
        let accumulatedText = ""
        let capturedSignature: string | null = null
        const capturedTools: any[] = []

        // Helper: Process a single parsed chunk
        const processChunk = (chunk: any) => {
            const candidate = chunk.candidates?.[0]
            const parts = candidate?.content?.parts || []

            for (const part of parts) {
                if (part.text) {
                    if (part.thought) {
                        // [GEMINI 3] Visual Thought Block (Markdown Safe)
                        const thoughtBlock = `\n> üß† **THINKING PROCESS**\n> ${part.text.replace(/\n/g, '\n> ')}\n\n`
                        accumulatedText += thoughtBlock
                    } else {
                        accumulatedText += part.text
                    }
                    responseText.setValue(accumulatedText)
                }

                // [ECHO PROTOCOL] Capture!
                if (part.thoughtSignature) {
                    console.log("üß† [Echo Protocol] Captured Signature Blob!")
                    capturedSignature = part.thoughtSignature
                    this.lastThoughtSignature = capturedSignature

                    // [A2UI FIX] Persist to localStorage for cross-refresh survival
                    try {
                        if (capturedSignature) {
                            localStorage.setItem(Gemini3Provider.SIG_STORAGE_KEY, capturedSignature)
                        }
                    } catch (e) {
                        // Ignore storage errors
                    }
                }

                // [VISUAL CORTEX] Image Blob Detection
                if (part.inlineData && part.inlineData.mimeType.startsWith('image/')) {
                    const mdImage = `\n![Generated Image](data:${part.inlineData.mimeType};base64,${part.inlineData.data})\n`
                    accumulatedText += mdImage
                    responseText.setValue(accumulatedText)
                    console.log("üé® [Gemini 3 Visual Cortex] Image Blob Captured & Rendered")
                }

                // [GEN UI 2.0] Widget Interception
                if (part.functionCall && part.functionCall.name === "render_widget") {
                    console.log("‚ö° [Gemini 3 Gen UI] Intercepting Widget Call:", part.functionCall.args)
                    const widgetPayload = {
                        type: "ui_component",
                        component: part.functionCall.args.component,
                        data: part.functionCall.args.data
                    }
                    const mdWidget = `\n\`\`\`json\n${JSON.stringify(widgetPayload, null, 2)}\n\`\`\`\n`
                    accumulatedText += mdWidget
                    responseText.setValue(accumulatedText)
                } else if (part.functionCall) {
                    // Normal Tool Call (Backend execution)
                    capturedTools.push({
                        name: part.functionCall.name,
                        arguments: part.functionCall.args
                    })
                }
            }
        }

        // [A2UI JSONL PARSER] Stream processing with bracket-depth tracking
        while (true) {
            const { done, value } = await reader.read()
            if (done) break

            buffer += decoder.decode(value, { stream: true })

            // Try to extract complete JSON objects from buffer
            // Gemini API streams as: [{...}\n,{...}\n,{...}]
            // We extract individual {...} objects
            let searchStart = 0
            while (searchStart < buffer.length) {
                const objStart = buffer.indexOf('{', searchStart)
                if (objStart === -1) break

                // Track bracket depth to find complete object
                let depth = 0
                let inString = false
                let escaped = false
                let objEnd = -1

                for (let i = objStart; i < buffer.length; i++) {
                    const char = buffer[i]

                    if (escaped) {
                        escaped = false
                        continue
                    }

                    if (char === '\\' && inString) {
                        escaped = true
                        continue
                    }

                    if (char === '"') {
                        inString = !inString
                        continue
                    }

                    if (!inString) {
                        if (char === '{') depth++
                        else if (char === '}') {
                            depth--
                            if (depth === 0) {
                                objEnd = i + 1
                                break
                            }
                        }
                    }
                }

                if (objEnd === -1) {
                    // Incomplete object, wait for more data
                    break
                }

                // Extract and parse complete object
                const jsonStr = buffer.substring(objStart, objEnd)
                try {
                    const chunk = JSON.parse(jsonStr)
                    processChunk(chunk)
                } catch (e) {
                    // Parse error, skip this segment
                    console.warn("üß† [Gemini 3] JSON parse error, skipping segment")
                }

                searchStart = objEnd
            }

            // Keep unparsed portion in buffer
            if (searchStart > 0) {
                buffer = buffer.substring(searchStart)
            }
        }

        // Finalize
        if (onFinal) {
            const finalMsg: Message = {
                id: crypto.randomUUID(),
                role: "model",
                content: accumulatedText,
                timestamp: Date.now(),
                customData: capturedSignature ? { thoughtSignature: capturedSignature } : undefined
            }

            if (capturedTools.length > 0) {
                finalMsg.content = "" // Tool calls usually have empty content
                finalMsg.tool_calls = capturedTools.map(fc => ({
                    id: "call_" + crypto.randomUUID().substring(0, 8),
                    name: fc.name,
                    arguments: fc.args
                }))
            }

            onFinal(finalMsg)
        }
    }

    private async runStream(runner: () => Promise<void>) {
        await runner()
    }
}
import { ObservableValue, DefaultObservableValue } from "@opendaw/lib-std"
import { LLMProvider, Message, ProviderConfig, LLMTool } from "./LLMProvider"
import { checkModelTier } from "./ModelPolicy"
import { ollamaInspector, ModelCapabilities } from "./OllamaCapabilityService"

export class OpenAICompatibleProvider implements LLMProvider {
    readonly id: string
    readonly manifest: {
        name: string
        description: string
        icon?: string
        getKeyUrl?: string
        docsUrl?: string
    }
    readonly requiresKey: boolean
    readonly requiresUrl: boolean

    private config: ProviderConfig = {}

    // [ANTIGRAVITY] Callback for persistence
    public onConfigChange?: (newConfig: ProviderConfig) => void

    // Cache for capabilities to avoid spamming /api/show
    private capabilityCache = new Map<string, ModelCapabilities>()

    constructor(
        id: string = "openai_compat",
        name: string = "OpenAI Compatible",
        defaultUrl: string = "",
        requiresKey: boolean = true,
        description: string = "Compatible with OpenRouter, Ollama, LM Studio, etc."
    ) {
        this.id = id
        this.manifest = {
            name,
            description
        }
        this.requiresKey = requiresKey

        // Specific manifest enhancements based on ID
        if (id === "ollama") {
            this.manifest.icon = "ü¶ô"
            this.manifest.getKeyUrl = "" // No key needed
            this.manifest.docsUrl = "https://ollama.com/"
        } else if (id === "openai") {
            this.manifest.icon = "üß†"
            this.manifest.getKeyUrl = "https://platform.openai.com/api-keys"
        } else if (id === "openrouter") {
            this.manifest.icon = "üåê"
            this.manifest.getKeyUrl = "https://openrouter.ai/keys"
        }

        // If a default URL is provided, set it immediately in a temp config so it's ready
        if (defaultUrl) {
            this.config.baseUrl = defaultUrl
        }

        // Always allow URL editing
        this.requiresUrl = true
    }

    async validate(): Promise<{ ok: boolean, message: string }> {
        try {
            const models = await this.fetchModels()
            // Special check for Ollama / Local
            if (models.length === 0) {
                // Check if we hit a CORS error (Browser enforcement)
                if (this.debugLog.includes("Failed to fetch") || this.debugLog.includes("NetworkError")) {
                    return {
                        ok: false,
                        message: "Network Error. If using Ollama, ensure `OLLAMA_ORIGINS=\"*\"` is set in your environment variables."
                    }
                }
                return { ok: false, message: "Connected, but no models found." }
            }
            return { ok: true, message: `Connected! Found: ${models.join(", ")}` }
        } catch (e: any) {
            return { ok: false, message: e.message || "Connection Failed" }
        }
    }

    configure(config: ProviderConfig) {
        this.config = config
    }

    streamChat(
        messages: Message[],
        _context?: any,
        tools?: LLMTool[],
        onFinal?: (msg: Message) => void
    ): ObservableValue<string> {
        const url = this.config.baseUrl || ""
        const key = this.config.apiKey || ""
        // Default model logic (Delayed until run for auto-detection)
        let model = this.config.modelId

        const responseText = new DefaultObservableValue<string>("[0/3] Initializing...")

        // Check for common misconfigurations
        if (!key && !url.includes("localhost") && !url.includes("127.0.0.1") && !url.startsWith("/")) {
            responseText.setValue("‚ö†Ô∏è API Key required for this endpoint.")
            return responseText
        }

        const run = async (): Promise<void> => {
            let targetUrl = url

            // Critical Fix: Ensure we hit the Chat API, not the root
            if (this.id === "ollama" && !targetUrl.includes("/api/chat") && !targetUrl.includes("/api/generate")) {
                let root = targetUrl
                if (root.endsWith("/")) root = root.slice(0, -1)
                if (root.endsWith("/v1")) root = root.slice(0, -3)
                targetUrl = `${root}/api/chat`
            }

            this.debugLog = `[${new Date().toISOString()}] Starting Chat Stream...\nURL: ${targetUrl} (Original: ${url})\nModel: ${model}\n`

            try {

                // Auto-detect model if missing (Crucial for first run)
                if (!model && this.id === "ollama") {
                    this.debugLog += `[Auto-Detect] No model configured. Scanning...\n`
                    try {
                        const models = await this.fetchModels()
                        if (models.length > 0) {
                            model = models[0]
                            this.debugLog += `[Auto-Detect] Success. Using: ${model}\n`
                            this.debugLog += `[Auto-Detect] Success. Using: ${model}\n`
                            this.config.modelId = model
                            // [ANTIGRAVITY] Persist the auto-detected model
                            if (this.onConfigChange) this.onConfigChange({ ...this.config })
                        } else {
                            this.debugLog += `[Auto-Detect] Failed. No models found.\n`
                        }
                    } catch (e: any) {
                        this.debugLog += `[Auto-Detect] Error: ${e.message}\n`
                    }

                    if (!model) {
                        this.debugLog += `[Fatal] No models found on server. Cannot proceed.\n`
                        responseText.setValue("‚ö†Ô∏è Connected to Ollama, but no models found.\nüëâ Please run: `ollama pull qwen2.5-coder`")
                        return
                    }
                }

                // --- CAPABILITY INSPECTION ---
                let capabilities: ModelCapabilities | null = null

                if (this.id === "ollama") {
                    // Check cache first
                    if (this.capabilityCache.has(model || "")) {
                        capabilities = this.capabilityCache.get(model || "")!
                    } else {
                        this.debugLog += `[Inspect] Analyzing physics for ${model}...\n`
                        capabilities = await ollamaInspector.inspect(targetUrl, model || "")
                        if (capabilities) {
                            this.capabilityCache.set(model || "", capabilities)
                            this.debugLog += `[Inspect] Result: ${capabilities.parameterSize} / ${capabilities.quantization} -> ${capabilities.suggestedTier}\n`
                        } else {
                            this.debugLog += `[Inspect] Failed to inspect. Falling back to name check.\n`
                        }
                    }
                }

                // --- WHITELIST ENFORCEMENT ---
                let activeTools = tools
                if (activeTools && activeTools.length > 0) {
                    const policy = checkModelTier(model || "unknown")
                    if (!policy.allowTools) {
                        console.warn(`[Policy] Model ${model} is Tier 3. Tools disabled.`)
                        this.debugLog += `[Policy] Tools disabled for Tier 3 model (${model}).\n`
                        activeTools = undefined
                    } else {
                        // Log Override usage if active
                        if (this.config.forceAgentMode) {
                            this.debugLog += `[Policy] ‚ö° MANUAL OVERRIDE ACTIVE. Tools enabled.\n`
                        }
                        this.debugLog += `[Policy] Tools enabled for Agent model (${model}).\n`
                    }
                }

                const controller = new AbortController()
                const timeoutId = setTimeout(() => controller.abort(), 60000) // 60s Timeout (Better for tools/local)

                this.debugLog += `Fetching...\n`

                // Construct Request Body
                const body: any = {
                    model: model,
                    messages: messages.map(m => ({
                        role: m.role === "model" ? "assistant" : (m.role === "system" ? "system" : "user"),
                        content: m.content
                    })),
                    stream: true
                }

                // Inject Tools if Allowed
                if (activeTools && activeTools.length > 0) {
                    body.tools = activeTools.map(t => ({
                        type: "function",
                        function: {
                            name: t.name,
                            description: t.description,
                            parameters: t.parameters
                        }
                    }))
                    // For Ollama/OpenAI, "auto" is default, but explicit is safer
                    // Only sent if tools exist
                }

                const response = await fetch(targetUrl, {
                    method: "POST",
                    signal: controller.signal,
                    headers: {
                        "Content-Type": "application/json",
                        "Authorization": `Bearer ${key}`,
                        "HTTP-Referer": "https://opendaw.studio",
                        "X-Title": "OpenDAW"
                    },
                    body: JSON.stringify(body)
                })
                clearTimeout(timeoutId)

                if (!response.ok) {
                    const err = await response.text()
                    this.debugLog += `Error Body: ${err}\n`
                    responseText.setValue(this.formatError(response.status, err))
                    return
                }

                if (!response.body) throw new Error("No response body")

                const reader = response.body.getReader()
                const decoder = new TextDecoder("utf-8")
                let accumulatedText = ""
                let toolCallsBuff: any[] = [] // Accumulate tool call fragments
                let buffer = ""

                while (true) {
                    const { done, value } = await reader.read()
                    if (value) {
                        const chunk = decoder.decode(value, { stream: true })
                        buffer += chunk
                    }

                    if (done) break

                    const lines = buffer.split("\n")
                    let tail = lines.pop() || ""

                    // JSON Blob Heuristics (Ollama Native)
                    if (tail && tail.trim().startsWith("{") && tail.trim().endsWith("}")) {
                        try { JSON.parse(tail); lines.push(tail); tail = ""; } catch (e) { }
                    }
                    buffer = tail

                    for (const line of lines) {
                        const trimmed = line.trim()
                        if (!trimmed) continue

                        let jsonStr = trimmed
                        if (trimmed.startsWith("data: ")) jsonStr = trimmed.substring(6)
                        if (jsonStr === "[DONE]") break

                        try {
                            // DEBUG: Log Raw Stream Chunk to catch malformed JSON from Local Models
                            // console.log("[Stream Raw]", jsonStr) 
                            if (jsonStr.startsWith("{")) console.log("[Stream Raw]", jsonStr.substring(0, 50)) // Log preview
                            const parsed = JSON.parse(jsonStr)

                            // 1. OpenAI / Ollama (Chat Endpoint)
                            const choice = parsed.choices?.[0]
                            const delta = choice?.delta

                            // Content (Standard)
                            if (delta?.content) {
                                accumulatedText += delta.content
                                responseText.setValue(accumulatedText)
                            }
                            // DeepSeek / Qwen "Thinking" (Standardized in some proxies)
                            if (delta?.thinking) {
                                accumulatedText += delta.thinking
                                responseText.setValue(accumulatedText)
                            }
                            // DeepSeek Reason (another variant)
                            if (delta?.reasoning_content) {
                                accumulatedText += delta.reasoning_content
                                responseText.setValue(accumulatedText)
                            }

                            // Tool Calls (Streamed fragments)
                            if (delta?.tool_calls) {
                                console.log("[Stream] Tool Call Delta:", JSON.stringify(delta.tool_calls))
                                delta.tool_calls.forEach((tc: any) => {
                                    const idx = tc.index
                                    if (!toolCallsBuff[idx]) toolCallsBuff[idx] = { id: "", name: "", arguments: "" }

                                    if (tc.id) toolCallsBuff[idx].id += tc.id
                                    if (tc.function?.name) toolCallsBuff[idx].name += tc.function.name
                                    if (tc.function?.arguments) toolCallsBuff[idx].arguments += tc.function.arguments
                                })
                            }

                            // Ollama Native (Generate Endpoint - unlikely used here but kept for compat)
                            const ollamaResponse = parsed.response // /api/generate
                            if (ollamaResponse) {
                                accumulatedText += ollamaResponse
                                responseText.setValue(accumulatedText)
                            }
                            const ollamaMessage = parsed.message // /api/chat (Non-streaming fallback)

                            // [ANTIGRAVITY] Support "Thinking" Models (Qwen/DeepSeek)
                            if (ollamaMessage?.thinking) {
                                // We append thinking as regular text for now to ensure visibility
                                accumulatedText += ollamaMessage.thinking
                                responseText.setValue(accumulatedText)
                            }

                            if (ollamaMessage?.content) {
                                accumulatedText += ollamaMessage.content
                                responseText.setValue(accumulatedText)
                            }
                            // Ollama Tool Calls (Non-streaming / final block)
                            if (ollamaMessage?.tool_calls) {
                                console.log("[Ollama] Raw Tool Calls:", ollamaMessage.tool_calls)
                                toolCallsBuff = ollamaMessage.tool_calls.map((tc: any) => ({
                                    id: "call_" + Math.random().toString(36).substr(2, 9),
                                    name: tc.function.name,
                                    arguments: JSON.stringify(tc.function.arguments) // Standardize to string for parsing later
                                }))
                            }

                        } catch (e) {
                            // JSON parse error (skip line)
                            // Only log if it's not a common "data: [DONE]" fragment
                            if (!jsonStr.includes("[DONE]")) {
                                console.warn("[Stream Parse Error]", e, jsonStr)
                            }
                        }
                    }
                }

                // FINALIZATION

                // Process Tool Calls
                const finalToolCalls = toolCallsBuff.map(tc => {
                    try {
                        return {
                            id: tc.id || "call_unknown",
                            name: tc.name,
                            arguments: JSON.parse(tc.arguments || "{}")
                        }
                    } catch (e) {
                        console.error("Failed to parse tool arguments", e)
                        return null
                    }
                }).filter(t => t !== null) as any[]

                this.debugLog += `[Done] Final Text: ${accumulatedText.length}, ToolCalls: ${finalToolCalls.length}\n`

                // Allow empty text IF there are tool calls
                if (!accumulatedText && finalToolCalls.length === 0) {
                    this.debugLog += `[WARN] No content extracted!\n`
                    responseText.setValue("‚ö†Ô∏è Connected, but received no text content.")
                    return
                }

                // Fire Final Callback
                if (onFinal) {
                    onFinal({
                        id: "final_" + Date.now(),
                        role: "model",
                        content: accumulatedText,
                        tool_calls: finalToolCalls.length > 0 ? finalToolCalls : undefined,
                        timestamp: Date.now()
                    })
                }

            } catch (e: any) {
                this.debugLog += `[Fatal] ${e.message}\n`
                console.error(e)

                // [ANTIGRAVITY] Enhanced CORS/Network Error Detection
                const isCORSorNetworkError = e instanceof TypeError || e.message === "Failed to fetch" || e.message.includes("NetworkError")

                if (isCORSorNetworkError && (targetUrl.includes("localhost") || targetUrl.includes("127.0.0.1"))) {
                    responseText.setValue(
                        `üö´ **Connection Blocked (CORS)**\n\n` +
                        `The browser blocked the request to Ollama at \`${targetUrl}\`.\n\n` +
                        `**To fix**, run Ollama with:\n` +
                        `\`\`\`bash\nOLLAMA_ORIGINS="*" ollama serve\n\`\`\`\n` +
                        `_Or use the '/api/ollama' proxy if available._`
                    )
                } else {
                    responseText.setValue(`Error: ${e.message}`)
                }
            }
        }

        this.runStream(async () => { await run() }).catch(e => {
            console.error("Critical Stream Failure", e)
            responseText.setValue(`Critical Error: ${e.message}`)
        })
        return responseText
    }

    // Captured debug info for the UI
    public debugLog: string = ""

    async fetchModels(): Promise<string[]> {
        const baseUrl = this.config.baseUrl || ""
        const key = this.config.apiKey || ""
        if (!baseUrl) return []

        const headers: any = {}
        if (key) headers["Authorization"] = `Bearer ${key}`

        let log = `--- Connection Diagnostics ---\nTimestamp: ${new Date().toISOString()}\nBaseURL: ${baseUrl}\n`

        const foundModels: Set<string> = new Set()

        // Strategy 1: OpenAI Standard (/v1/models)
        try {
            // Adjust URL: If it ends in /chat/completions, strip it to find root
            let cleanUrl = baseUrl.replace("/chat/completions", "")
            if (cleanUrl.endsWith("/v1")) cleanUrl = cleanUrl
            else if (!cleanUrl.endsWith("/v1")) cleanUrl = cleanUrl.endsWith("/") ? `${cleanUrl}v1` : `${cleanUrl}/v1`

            const targetUrl = `${cleanUrl}/models`
            log += `\n[Strategy 1] GET ${targetUrl}`

            const controller = new AbortController()
            const timeout = setTimeout(() => controller.abort(), 10000)

            try {
                const res = await fetch(targetUrl, { headers, signal: controller.signal })
                clearTimeout(timeout)
                log += `\nStatus: ${res.status} ${res.statusText}`

                if (res.ok) {
                    const text = await res.text()
                    try {
                        const data = JSON.parse(text)
                        log += `\nBody: ${JSON.stringify(data, null, 2).slice(0, 500)}...`

                        if (Array.isArray(data.data)) {
                            // OpenAI Standard
                            data.data.forEach((m: any) => foundModels.add(m.id))
                        }
                        // Ollama sometimes leaks 'models' here too
                        if (Array.isArray(data.models)) {
                            data.models.forEach((m: any) => foundModels.add(m.name || m.id))
                        }
                    } catch (e) {
                        log += `\nParse Error: ${text.slice(0, 200)}`
                    }
                }
            } catch (e: any) {
                log += `\nError: ${e.message}`
            }

            // Strategy 2: Ollama Standard (/api/tags)
            // Ollama usually runs on port 11434, root is often just http://localhost:11434
            // If user passed .../v1 or .../api/chat, strip it.
            // If user passed .../v1 or .../api/chat, strip it.
            const rootUrl = baseUrl
                .replace(/\/v1\/chat\/completions\/?$/, "")
                .replace(/\/api\/chat\/?$/, "")
                .replace(/\/v1\/?$/, "")
            const targetUrl2 = `${rootUrl}/api/tags`
            log += `\n\n[Strategy 2] GET ${targetUrl2}`

            const controller2 = new AbortController()
            const timeout2 = setTimeout(() => controller2.abort(), 10000)

            try {
                const res = await fetch(targetUrl2, { signal: controller2.signal })
                clearTimeout(timeout2)
                log += `\nStatus: ${res.status} ${res.statusText}`

                if (res.ok) {
                    const text = await res.text()
                    try {
                        const data = JSON.parse(text)
                        log += `\nBody: ${JSON.stringify(data, null, 2).slice(0, 500)}...`

                        if (Array.isArray(data.models)) {
                            // Support 'name' (Ollama) and 'id' (Generic)
                            data.models.forEach((m: any) => foundModels.add(m.name || m.id || m.model))
                        }
                    } catch (err) {
                        log += `\nParse Error: ${text.slice(0, 200)}`
                    }
                } else {
                    log += `\nResponse Not OK: ${res.status}`
                }
            } catch (e: any) {
                const isNetwork = e.message.includes("fetch") || e.name === "TypeError"
                const hint = isNetwork ? " (Check CORS/OLLAMA_ORIGINS)" : ""
                log += `\nError: ${e.message}${hint}`
            }

            // Strategy 3: Auto-Detect / Fallback (The "Magic" Fix)
            // If we haven't found models yet, and we are Ollama, try known standard endpoints regardless of config.
            if (foundModels.size === 0 && this.id === "ollama") {
                const fallbacks = [
                    "/api/ollama", // Proxy-First: Avoid CORS
                    "http://localhost:11434", // Direct: Fallback
                ]

                for (const fallbackBase of fallbacks) {
                    // Avoid re-testing the configured base if it matches
                    const cleanBase = baseUrl.replace(/\/api\/chat\/?$/, "").replace(/\/v1\/?$/, "")
                    if (fallbackBase === cleanBase) continue

                    const fallbackUrl = `${fallbackBase}/api/tags`
                    log += `\n\n[Strategy 3] Auto-Detect GET ${fallbackUrl}`

                    try {
                        const controller3 = new AbortController()
                        const timeout3 = setTimeout(() => controller3.abort(), 2000) // Fast fail
                        const res = await fetch(fallbackUrl, { signal: controller3.signal })
                        clearTimeout(timeout3)

                        log += `\nStatus: ${res.status}`
                        if (res.ok) {
                            const text = await res.text()
                            const data = JSON.parse(text)
                            if (Array.isArray(data.models)) {
                                data.models.forEach((m: any) => foundModels.add(m.name || m.id || m.model))
                                if (foundModels.size > 0) {
                                    log += `\nüéØ Auto-Detect Success! Switching baseUrl to: ${fallbackBase}`
                                    // [ANTIGRAVITY] AUTO-HEAL: Update the instance URL so chat works
                                    // Note: This overrides the user's bad config for this session.
                                    this.config.baseUrl = fallbackBase
                                    // [ANTIGRAVITY] AUTO-HEAL: Persist the working URL
                                    if (this.onConfigChange) this.onConfigChange({ ...this.config })
                                    break // Stop trying fallbacks
                                    break // Stop trying fallbacks
                                }
                            }
                        }
                    } catch (e: any) {
                        log += `\nFallback Error: ${e.message}`
                    }
                }
            }

        } catch (e: any) {
            log += `\nOuter Error: ${e.message}`
        }

        log += `\n\nTotal Models Found: ${foundModels.size}`
        this.debugLog = log
        console.log(this.debugLog)

        return Array.from(foundModels)
    }

    private async runStream(runner: () => Promise<void>) {
        await runner()
    }

    /**
     * Convert API errors into friendly, helpful messages
     */
    private formatError(status: number, rawText: string): string {
        try {
            // Robust Parsing: Handle non-JSON responses gracefully
            let errorObj: any = {}
            if (rawText.trim().startsWith("{")) {
                try {
                    const parsed = JSON.parse(rawText)
                    errorObj = parsed.error || parsed
                } catch (e) {
                    // Partial JSON? Use raw
                    errorObj = { message: rawText }
                }
            } else {
                errorObj = { message: rawText }
            }

            const message = errorObj?.message || ''

            // Rate Limit
            if (status === 429 || message.toLowerCase().includes('rate')) {
                return `‚è≥ **Too Many Requests**\n\nSlow down a bit! Wait a moment and try again.`
            }

            // Auth Issues
            if (status === 401 || status === 403) {
                return `üîë **Access Denied**\n\nCheck your API key in Settings.`
            }

            // Not Found (Model or Endpoint)
            if (status === 404) {
                return "```json\n" + JSON.stringify({
                    ui_component: "error_card",
                    data: {
                        title: "Model Not Found",
                        message: "We couldn't connect to the configured AI model. Please check that Ollama is running and the model is installed.",
                        actions: [
                            { label: "‚öôÔ∏è Open Settings", id: "open_settings" }
                        ]
                    }
                }, null, 2) + "\n```"
            }

            // Server Errors
            if (status >= 500) {
                return `üîß **Server Issue**\n\nThe AI service is having problems. Try again in a minute.`
            }

            // Generic with parsed message
            if (message) {
                // Formatting: Capitalize first letter
                const formatted = message.charAt(0).toUpperCase() + message.slice(1)
                return `‚ö†Ô∏è **Error ${status}**: ${formatted}`
            }
        } catch (e) {
            // JSON parsing failed completely
            return `‚ö†Ô∏è **Error (${status})**\n\nServer returned invalid response.`
        }

        return `‚ö†Ô∏è **Error (${status})**\n\nSomething went wrong. Please try again.`
    }
}
